{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1913658,"sourceType":"datasetVersion","datasetId":1036526},{"sourceId":8689767,"sourceType":"datasetVersion","datasetId":5210469},{"sourceId":65616,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":54738},{"sourceId":65629,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":54750},{"sourceId":65665,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":54784},{"sourceId":65703,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":54815}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torchvision\nimport torchvision.models as models\nimport torch.nn.functional as F\nfrom torchvision.io import read_image\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nimport zipfile\nimport math\nfrom skimage.metrics import peak_signal_noise_ratio, structural_similarity\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport seaborn as sns","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-15T19:04:21.160661Z","iopub.execute_input":"2024-06-15T19:04:21.161233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_path = \"/kaggle/input/landscape/landscape Images\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MANUAL_SEED = 42\nBATCH_SIZE = 32\nWIDTH = 150\nHEIGHT = 150\nSHUFFLE = True\nTRAINING_SIZE = 0.8","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\nclass LandscapeDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.dataroot = root_dir\n        self.images = os.listdir(f'{self.dataroot}/color')\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img_path = self.images[idx]\n\n        color_img = Image.open(f'{self.dataroot}/color/{img_path}').convert('RGB')\n        gray_img = Image.open(f'{self.dataroot}/gray/{img_path}').convert('L')\n\n        if self.transform:\n            color_img = self.transform(color_img)\n            gray_img = self.transform(gray_img)\n\n        return color_img, gray_img\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((WIDTH, HEIGHT)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n    transforms.ToTensor()\n])\n\ndataset = LandscapeDataset(root_dir=dataset_path, transform=transform)\n\n# Split the data into train and test data\ntrain_set, test_set = random_split(\n    dataset, \n    [int(TRAINING_SIZE * len(dataset)), len(dataset) - int(TRAINING_SIZE * len(dataset))], \n    generator=torch.Generator().manual_seed(MANUAL_SEED)\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainloader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=SHUFFLE)\ntestloader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=SHUFFLE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_images(color, gray):\n    fig, axs = plt.subplots(5, 2, figsize=(15, 15))\n    axs[0, 0].set_title('Grayscale')\n    axs[0, 1].set_title('Color')\n    for i in range(5):\n        axs[i, 0].imshow(gray[i].permute(1, 2, 0), cmap='gray')\n        axs[i, 0].axis('off')\n        axs[i, 1].imshow(color[i].permute(1, 2, 0))\n        axs[i, 1].axis('off')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"color, gray = next(iter(trainloader))\nshow_images(color, gray)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 3\nLEARNING_RATE = 0.001\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nDEVICE","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EarlyStopping:\n    def __init__(self, monitor, patience=3, verbose=False, delta=0, path='checkpoint.pt', max_accuracy=0.95, trace_func=print):\n        self.monitor = monitor\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.acc_max = -np.Inf\n        self.delta = delta\n        self.path = path\n        self.max_accuracy = max_accuracy\n        self.trace_func = trace_func\n\n    def __call__(self, acc, model):\n        score = acc\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(acc, model)\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            if self.verbose:\n                self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(acc, model)\n            self.counter = 0\n\n        if acc >= self.max_accuracy:\n            self.early_stop = True\n            if self.verbose:\n                self.trace_func(f'Maximum accuracy of {self.max_accuracy} reached. Stopping training.')\n\n    def save_checkpoint(self, acc, model):\n        if self.verbose:\n            self.trace_func(f'Accuracy increased ({self.acc_max} --> {acc}).  Saving model ...')\n        torch.save(model.state_dict(), self.path)\n        self.acc_max = acc\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ColorAutoEncoder(nn.Module):\n    def __init__(self):\n        super(ColorAutoEncoder, self).__init__()\n        densenet = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n        self.encoder = nn.Sequential(\n            nn.Conv2d(1, 3, kernel_size=3, stride=1, padding=1, bias=False),  # Adjusting input channel to 1\n            *list(densenet.features.children())  # Use the features part of DenseNet121\n        )\n        \n        # Define the decoder architecture\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(1024, 512, kernel_size=3, stride=2, padding=1, output_padding=1),\n            nn.ReLU(),\n            nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, output_padding=1),\n            nn.ReLU(),\n            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n            nn.ReLU(),\n            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n            nn.ReLU(),\n            nn.ConvTranspose2d(64, 3, kernel_size=3, stride=2, padding=1, output_padding=1),\n            nn.Sigmoid()  # Using sigmoid to map the output between 0 and 1\n        )\n        \n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return x\n\nmodel = ColorAutoEncoder().to(DEVICE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_path = '/kaggle/input/checkpoint/pytorch/v20/1/'\nfiles = os.listdir(model_path)\nif files:\n    print(\"Loading model...\")\n    model.load_state_dict(torch.load(model_path+\"checkpoint.pt\", map_location=torch.device(DEVICE)))\n    print(f\"Model successfully loaded on {DEVICE}.\")\n    \ntorch.save(model.state_dict(), 'color_autoencoder.pth ')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PerceptualLoss(nn.Module):\n    def __init__(self, feature_layer=9):\n        super(PerceptualLoss, self).__init__()\n        vgg = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1).features\n        self.feature_extractor = nn.Sequential(*list(vgg)[:feature_layer]).eval()\n        for param in self.feature_extractor.parameters():\n            param.requires_grad = False\n\n    def forward(self, pred, target):\n        pred_features = self.feature_extractor(pred)\n        target_features = self.feature_extractor(target)\n        return F.mse_loss(pred_features, target_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Total Number of trainable parameters of this model are: {total_params:,}\")\n\ncriterion = PerceptualLoss().to(DEVICE)\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_losses = []\nval_losses = []\ntrain_accuracies = []\nval_accuracies = []","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = 'v21_checkpoint.pt'\nearly_stopper = EarlyStopping(monitor='accuracy', max_accuracy=0.8, patience=60, verbose=True, path=model_name)\nMAX_EPOCHS = 0\n\nfor epoch in range(EPOCHS):\n    \n#     Start Training\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n#     Get gray image + its colored version\n    for idx, (color_img, gray_img) in tqdm(enumerate(trainloader), total=len(trainloader)):\n        \n#         Send image to GPU\n        color_img = color_img.to(DEVICE)\n        gray_img = gray_img.float().to(DEVICE)\n\n#         Resize prediction to height x width\n        predictions = model(gray_img)\n        predictions = F.interpolate(\n                predictions, \n                size=(WIDTH, HEIGHT), \n                mode='bilinear', \n                align_corners=True\n            )\n\n        optimizer.zero_grad() # Clear past gradient\n        loss = criterion(color_img, predictions) # calculate loss\n        loss.backward() # calculate loss gradient\n        optimizer.step() # optimize parameters\n\n        running_loss += loss.item() # accumulate total loss\n\n#         get prediction class for each pixel\n        predicted_classes = torch.argmax(predictions, dim=1) \n        true_classes = torch.argmax(color_img, dim=1)\n        \n        correct += (predicted_classes == true_classes).sum().item() # move all correctly predicted classes to correct\n        total += true_classes.numel()\n\n    train_losses.append(running_loss / len(trainloader)) # append average loss\n    train_accuracies.append(correct / total) # append accuracy\n\n    # Start Validation\n    model.eval()\n    val_running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad(): # disable gradient calculation\n        #     Get gray image + its colored version\n        for idx, (color_img, gray_img) in tqdm(enumerate(testloader), total=len(testloader)):\n            \n            #         Send image to GPU\n            color_img = color_img.to(DEVICE)\n            gray_img = gray_img.to(DEVICE)\n\n            #         Resize prediction to height x width\n            predictions = model(gray_img)\n            predictions = F.interpolate(\n                predictions, \n                size=(WIDTH, HEIGHT), \n                mode='bilinear', \n                align_corners=True\n            )\n\n            loss = criterion(predictions, color_img) # calculate loss\n            val_running_loss += loss.item() # accumulate validation loss\n\n            #         get prediction class for each pixel\n            predicted_classes = torch.argmax(predictions, dim=1)\n            true_classes = torch.argmax(color_img, dim=1)\n            \n            \n            correct += (predicted_classes == true_classes).sum().item() # move all correctly predicted classes to correct\n            total += true_classes.numel()\n\n    val_losses.append(val_running_loss / len(testloader)) # append average validation loss\n    val_accuracies.append(correct / total) # append validation accuracy\n\n    # Print and/or log metrics after each epoch\n    print(\n        f\"Epoch: {epoch + 1} / {EPOCHS}, \"\n        f\"Train Acc: {train_accuracies[-1]}, \"\n        f\"Val Acc: {val_accuracies[-1]}, \"\n        f\"Train Loss: {train_losses[-1]}, \"\n        f\"Val Loss: {val_losses[-1]}\"\n    )\n    \n    MAX_EPOCHS = epoch + 1\n    early_stopper(train_accuracies[-1], model)\n\n    if early_stopper.early_stop:\n        print(\"Early stopping\")\n        break\n\n#     Adjust learning rate if using scheduler\n    scheduler.step()\n\nmodel.load_state_dict(torch.load(model_name, map_location=torch.device(DEVICE)))\nprint('Training Finished!')\ntorch.save(model.state_dict(), 'color_autoencoder.pth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EnhancedColorAutoEncoder(nn.Module):\n    def __init__(self):\n        super(EnhancedColorAutoEncoder, self).__init__()\n        # Load the pre-trained ColorAutoEncoder model\n        self.color_autoencoder = ColorAutoEncoder()\n        self.color_autoencoder.load_state_dict(torch.load('color_autoencoder.pth', map_location=DEVICE))\n\n        # Freeze the parameters of the ColorAutoEncoder to retain learned features\n        for param in self.color_autoencoder.parameters():\n            param.requires_grad = False\n        \n        self.down1 = nn.Conv2d(1, 64, 3, stride=2)\n        self.down2 = nn.Conv2d(64, 128, 3, stride=2, padding=1)\n        self.down3 = nn.Conv2d(128, 256, 3, stride=2, padding=1)\n        self.down4 = nn.Conv2d(256, 512, 3, stride=2, padding=1)\n        \n        self.up0 = nn.ConvTranspose2d(3, 512, kernel_size=3, stride=1, padding=1),\n        self.up1 = nn.ConvTranspose2d(512, 256, 3, stride=2, padding=1)\n        self.up2 = nn.ConvTranspose2d(512, 128, 3, stride=2, padding=1)\n        self.up3 = nn.ConvTranspose2d(256, 64, 3, stride=2, padding=1, output_padding=1)\n        self.up4 = nn.ConvTranspose2d(128, 3, 3, stride=2, output_padding=1)\n        \n        self.relu = nn.ReLU()\n        self.sigmoid = nn.Sigmoid()      \n        \n\n    def forward(self, x):\n        d1 = self.relu(self.down1(x))\n        d2 = self.relu(self.down2(d1))\n        d3 = self.relu(self.down3(d2))\n        d4 = self.relu(self.down4(d3))\n\n        u1 = self.relu(self.up1(d4))\n        u2 = self.relu(self.up2(torch.cat((u1, d3), dim=1)))\n        u3 = self.relu(self.up3(torch.cat((u2, d2), dim=1)))\n        u4 = self.sigmoid(self.up4(torch.cat((u3, d1), dim=1)))\n\n        return u4\n\n# Initialize the new model\nenhanced_model = EnhancedColorAutoEncoder().to(DEVICE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define a loss function and optimizer\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(enhanced_model.parameters(), lr=0.001)\n\n# Training loop\nfor epoch in range(1):\n    enhanced_model.train()\n    running_loss = 0.0\n    \n    for idx, (color_img, gray_img) in tqdm(enumerate(trainloader), total=len(trainloader)):\n        color_img = color_img.to(DEVICE)\n        gray_img = gray_img.float().to(DEVICE)\n        \n        optimizer.zero_grad()\n        \n        outputs = enhanced_model(gray_img)\n        outputs = F.interpolate(\n                outputs, \n                size=(WIDTH, HEIGHT), \n                mode='bilinear', \n                align_corners=True\n            )\n        \n        loss = criterion(outputs, color_img)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n    \n    print(f\"Epoch [{epoch+1}/{EPOCHS}], Loss: {running_loss / len(trainloader)}\")\n\n# Save the enhanced model\ntorch.save(enhanced_model.state_dict(), 'enhanced_color_autoencoder.pth')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_psnr(img1, img2):\n    return peak_signal_noise_ratio(img1, img2, data_range=img2.max() - img2.min())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_loss = 0.0\ntotal_psnr = 0.0\ntotal_ssim = 0.0\nall_true_classes = []\nall_predicted_classes = []","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n    for idx, (color_img, gray_img) in tqdm(enumerate(testloader), total=len(testloader)):\n        color_img = color_img.to(DEVICE)\n        gray_img = gray_img.to(DEVICE)\n\n        prediction = enhanced_model(gray_img)\n        prediction = F.interpolate(\n                prediction, \n                size=(WIDTH, HEIGHT), \n                mode='bilinear', \n                align_corners=True\n            )\n\n        loss = criterion(prediction, color_img)\n        total_loss += loss.item()\n\n        psnr = calculate_psnr(color_img.cpu().numpy(), prediction.cpu().numpy())\n        total_psnr += psnr\n\n        predicted_classes = torch.argmax(prediction, dim=1)\n        true_classes = torch.argmax(color_img, dim=1)\n        all_true_classes.extend(true_classes.cpu().numpy().flatten())\n        all_predicted_classes.extend(predicted_classes.cpu().numpy().flatten())\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"avg_psnr = total_psnr / len(testloader)\n\nprint(f\"Total Testing loss is: {total_loss / len(testloader)}\")\nprint(f\"Average PSNR: {avg_psnr}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute confusion matrix\nconf_matrix = confusion_matrix(all_true_classes, all_predicted_classes)\n\n# Compute accuracy\naccuracy = accuracy_score(all_true_classes, all_predicted_classes)\n\nprint(f\"Confusion Matrix:\\n{conf_matrix}\")\nprint(f\"Accuracy: {accuracy}\")\n\n# Plot confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, cmap=\"Blues\", fmt=\"d\", xticklabels=True, yticklabels=True)\nplt.xlabel('Predicted labels')\nplt.ylabel('True labels')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_predictions(color, gray, pred):\n    fig, axs = plt.subplots(5, 3, figsize=(15, 15))\n    axs[0, 0].set_title('Grayscale')\n    axs[0, 1].set_title('Color')\n    axs[0, 2].set_title('Predicted')\n    for i in range(5):\n        axs[i, 0].axis('off')\n        axs[i, 0].imshow(gray[i].permute(1, 2, 0), cmap='gray')\n        axs[i, 1].axis('off')\n        axs[i, 1].imshow(color[i].permute(1, 2, 0))\n        axs[i, 2].axis('off')\n        axs[i, 2].imshow(pred[i].permute(1, 2, 0))\n    plt.show()\n\nshow_predictions(color_img.detach().cpu(), gray_img.detach().cpu(), prediction.detach().cpu())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = range(MAX_EPOCHS)\n\nplt.figure(figsize=(15, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(epochs, train_accuracies, label='Train Set')\nplt.plot(epochs, val_accuracies, label='Val Set')\nplt.title('Model Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs, train_losses, label='Train Set')\nplt.plot(epochs, val_losses, label='Val Set')\nplt.title('Model Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}